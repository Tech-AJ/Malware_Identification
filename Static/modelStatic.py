import numpy as np
import pandas as pd

data_train = pd.read_csv('staticTrain.csv')

for column in data_train.columns:
    data_train[column].fillna(data_train[column].mode()[0], inplace=True)

from sklearn.model_selection import train_test_split

X_all = data_train.drop(['Malware'], axis=1)
y_all = data_train['Malware']

# final training on 100% data
num_test = 0.0
X_train, X_test, y_train, y_tst = train_test_split(X_all, y_all, test_size=num_test)

from sklearn.ensemble import RandomForestClassifier
from sklearn.metrics import make_scorer, accuracy_score
from sklearn.model_selection import GridSearchCV

# Choose the type of classifier.
clf = RandomForestClassifier()


# Choose some parameter combinations to try for GridSearch
parameters = {'n_estimators': [30, 40, 20],
              'max_features': ['log2', 'sqrt', 'auto'],
              'criterion': ['entropy', 'gini'],
              'max_depth': [2, 3, 5, 10],
              'min_samples_split': [2, 3, 5],
              'min_samples_leaf': [1, 5, 8]
              }

# Type of scoring used to compare parameter combinations
acc_scorer = make_scorer(accuracy_score)

# Run the grid search for hyperparameter tuning
grid_obj = GridSearchCV(clf, parameters, scoring=acc_scorer)
grid_obj = grid_obj.fit(X_train, y_train)

# Set the clf to the best combination of parameters
clf = grid_obj.best_estimator_
print(grid_obj.best_estimator_)
print(clf)

# from joblib import dump, load
# dump(clf, 'modelStatic.joblib')

#store model
import pickle
pickle.dump(clf, open("modelStatic.sav", 'wb'))


# run Kfold
from sklearn.model_selection import KFold
from sklearn import metrics


def run_kfold(clf):
    kf = KFold(n_splits=5, shuffle=True)
    outcomes = []
    fold = 0
    for train_index, test_index in kf.split(data_train):
        fold += 1
        X_train, X_test = X_all.values[train_index], X_all.values[test_index]
        y_train, y_test = y_all.values[train_index], y_all.values[test_index]
        clf.fit(X_train, y_train)
        predictions = clf.predict(X_test)
        accuracy = accuracy_score(y_test, predictions)
        outcomes.append(accuracy)
        print("Fold {0} accuracy: {1}".format(fold, accuracy))
        print(metrics.classification_report(y_test, predictions))
        print(metrics.confusion_matrix(y_test, predictions))
    mean_outcome = np.mean(outcomes)
    print("Mean Accuracy: {0}".format(mean_outcome))


run_kfold(clf)


# RandomForest values  used in model from gridsearch
# RandomForestClassifier(bootstrap=True, class_weight=None, criterion='gini',
#             max_depth=10, max_features='auto', max_leaf_nodes=None,
#             min_impurity_decrease=0.0, min_impurity_split=None,
#             min_samples_leaf=1, min_samples_split=2,
#             min_weight_fraction_leaf=0.0, n_estimators=40, n_jobs=None,
#             oob_score=False, random_state=None, verbose=0,
#             warm_start=False)
